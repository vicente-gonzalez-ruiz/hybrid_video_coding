<!DOCTYPE html> 
<html lang='en-US' xml:lang='en-US'> 
<head> <title>Video Compression</title> 
<meta charset='utf-8' /> 
<meta content='TeX4ht (https://tug.org/tex4ht/)' name='generator' /> 
<meta content='width=device-width,initial-scale=1' name='viewport' /> 
<link href='index.css' rel='stylesheet' type='text/css' /> 
<meta content='index.tex' name='src' /> 
<script>window.MathJax = { tex: { tags: "ams", }, }; </script> 
 <script async='async' id='MathJax-script' src='https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-chtml-full.js' type='text/javascript'></script>  
</head><body>
   <div class='maketitle'>
                                                                  

                                                                  
                                                                  

                                                                  

<h2 class='titleHead'>Video Compression</h2>
 <div class='author'><span class='ecrm-1200'>Vicente González Ruiz</span></div><br />
<div class='date'><span class='ecrm-1200'>January 11, 2023</span></div>
   </div>
   <h3 class='likesectionHead' id='contents'><a id='x1-1000'></a>Contents</h3>
   <div class='tableofcontents'>
    <span class='sectionToc'>1 <a href='#-httpsvicentegonzalezruizgithubioredundancysources-of-redundancy' id='QQ2-1-2'>Sources of redundancy</a></span>
<br />     <span class='subsectionToc'>1.1 <a href='#intra-i-video-coding' id='QQ2-1-3'>Intra (I) video coding</a></span>
<br />    <span class='sectionToc'>2 <a href='#-the-ipp-mrvc-encoder-' id='QQ2-1-5'>Hybrid video coding</a></span>
<br />     <span class='subsectionToc'>2.1 <a href='#motion-compensation-prediction-p-video-coding' id='QQ2-1-7'>Motion compensation prediction (P) video coding</a></span>
<br />     <span class='subsectionToc'>2.2 <a href='#types-of-macroblocks' id='QQ2-1-8'>Types of macro-blocks</a></span>
<br />     <span class='subsectionToc'>2.3 <a href='#macroblock-type-decision' id='QQ2-1-10'>Macro-block type decision</a></span>
<br />     <span class='subsectionToc'>2.4 <a href='#codec' id='QQ2-1-11'>Codec</a></span>
<br />     <span class='subsectionToc'>2.5 <a href='#a-block-diagram-of-the-step-codec' id='QQ2-1-13'>A block diagram of the step codec</a></span>
<br />    <span class='sectionToc'>3 <a href='#blockbased-mc-motion-compensation-raotechniques' id='QQ2-1-15'>Block-based MC (Motion Compensation) [2]</a></span>
<br />    <span class='sectionToc'>4 <a href='#subpixel-accuracy' id='QQ2-1-17'>Sub-pixel accuracy</a></span>
<br />    <span class='sectionToc'>5 <a href='#matching-criteria-similitude-between-macroblocks' id='QQ2-1-19'>Matching criteria (similitude between macroblocks)</a></span>
<br />    <span class='sectionToc'>6 <a href='#searching-strategies' id='QQ2-1-20'>Searching strategies</a></span>
<br />    <span class='sectionToc'>7 <a href='#the-gop-group-of-pictures-concept' id='QQ2-1-22'>The GOP (Group Of Pictures) concept</a></span>
<br />    <span class='sectionToc'>8 <a href='#mctf-motion-compensated-temporal-filtering' id='QQ2-1-24'>MCTF (Motion Compensated Temporal Filtering)</a></span>
<br />    <span class='sectionToc'>9 <a href='#-spiralsearch-me-motion-estimation' id='QQ2-1-25'>\(\pm \) 1-spiral-search ME (Motion Estimation)</a></span>
<br />    <span class='sectionToc'>10 <a href='#linear-frame-interpolation-using-blockbased-motion-compensation' id='QQ2-1-27'>Linear frame interpolation using block-based motion compensation</a></span>
<br />    <span class='sectionToc'>11 <a href='#mcdwt-hybrid-coding-alternatives' id='QQ2-1-41'>MC/DWT hybrid coding alternatives</a></span>
<br />    <span class='sectionToc'>12 <a href='#deblocking-filtering' id='QQ2-1-43'>Deblocking filtering</a></span>
<br />    <span class='sectionToc'>13 <a href='#bitrate-allocation' id='QQ2-1-45'>Bit-rate allocation</a></span>
<br />    <span class='sectionToc'>14 <a href='#video-scalability' id='QQ2-1-48'>Video scalability</a></span>
<br />     <span class='subsectionToc'>14.1 <a href='#quality-scalability' id='QQ2-1-49'>Quality scalability</a></span>
<br />     <span class='subsectionToc'>14.2 <a href='#temporal-scalability' id='QQ2-1-51'>Temporal scalability</a></span>
<br />     <span class='subsectionToc'>14.3 <a href='#-httpinsteecsberkeleyedu-eetsplecturesvideowaveletucbpdfspatial-scalability' id='QQ2-1-53'>Spatial scalability</a></span>
<br />    <span class='sectionToc'>15 <a href='#coding-in-the-transform-domain' id='QQ2-1-55'>Coding in the Transform Domain</a></span>
<br />     <span class='subsectionToc'>15.1 <a href='#the-ipp-decorrelation-pattern' id='QQ2-1-56'>The IPP... decorrelation pattern</a></span>
                                                                  

                                                                  
<br />     <span class='subsectionToc'>15.2 <a href='#a-block-diagram-of-the-step-codec1' id='QQ2-1-57'>A block diagram of the step codec</a></span>
<br />     <span class='subsectionToc'>15.3 <a href='#spatial-multiresolution' id='QQ2-1-59'>Spatial multiresolution</a></span>
<br />     <span class='subsectionToc'>15.4 <a href='#sq-layers-progression-next-milestone' id='QQ2-1-62'>SQ (layers) progression (next milestone?)</a></span>
<br />    <span class='sectionToc'>16 <a href='#references' id='QQ2-1-63'>References</a></span>
   </div>
<!-- l. 6 --><p class='noindent'>
</p>
   <h3 class='sectionHead' id='-httpsvicentegonzalezruizgithubioredundancysources-of-redundancy'><span class='titlemark'>1   </span> <a id='x1-20001'></a><a href='https://vicente-gonzalez-ruiz.github.io/redundancy/'>Sources of redundancy</a></h3>
<!-- l. 8 --><p class='noindent'>
</p>
   <h4 class='subsectionHead' id='intra-i-video-coding'><span class='titlemark'>1.1   </span> <a id='x1-30001.1'></a>Intra (I) video coding</h4>
<!-- l. 10 --><p class='noindent'>In the III... (or Intra video) coding, the <a href='https://sistemas-multimedia.github.io/milestones/07-DCT/'>2D block-DWT</a>, the <a href='https://sistemas-multimedia.github.io/milestones/08-DWT/'>2D DWT</a>, or
any other spatial transform, is used on sequences of frames (images)
to exploit the spatial correlation. This is achieved by simply iterating the
spatial decorrelation as it is described in the Algorithm <a href='#x1-3002r1'>1<!-- tex4ht:ref: alg:III_coding  --></a> <span class='cite'>[<a href='#Xtaubman2002jpeg2000'>3</a>]</span>, where \(V\) in
the input sequence and \(S\) controls the number of SRLs (Spatial Resolution
Levels)<span class='footnote-mark'><a href='#fn1x0' id='fn1x0-bk'><sup class='textsuperscript'>1</sup></a></span><a id='x1-3001f1'></a>.
The synthesis transform is computed using the Algorithm <a href='#x1-3009r2'>2<!-- tex4ht:ref: alg:III_decoding  --></a>. In the Fig. <a href='#x1-3016r1'>1<!-- tex4ht:ref: fig:III  --></a>
there is an example of the decomposition generated for three frames \(V_0\), \(V_1\) and
\(V_2\).
<a id='x1-3002r1'></a>
</p><!-- l. 25 --><p class='indent'>   <span class='ecbx-1000'>Algorithm 1</span>: III-coding(\(\mathbf {V}\) /* original video sequence */, \(S\) /* Number of extra
levels */) \(\rightarrow \) (\(\mathbf {O}\) /* transformed video sequence */)<br class='newline' />
     </p><ol class='enumerate1'>
<li class='enumerate' id='x1-3004x1'>\({\mathbf O}=\{\}\) /* empty sequence */.
     </li>
<li class='enumerate' id='x1-3006x2'>
     <!-- l. 29 --><p class='noindent'>for \({\mathbf V}_i\in {\mathbf V}\):
         </p><ol class='enumerate2'>
<li class='enumerate' id='x1-3008x1'>\({\mathbf O}_i\leftarrow \text {2D-T}^{S}({\mathbf V}_i)\) /* 2D analysis spatial transform */.</li></ol>
     </li></ol>
                                                                  

                                                                  
<p><a id='x1-3009r2'></a>
</p><!-- l. 36 --><p class='indent'>   <span class='ecbx-1000'>Algorithm 2</span>: III-decoding(\(\mathbf {O}\) /* transformed video sequence */, \(S\) /* Number of
extra levels */) \(\rightarrow \) (\(\mathbf {V}\) /* original video sequence */)<br class='newline' />
     </p><ol class='enumerate1'>
<li class='enumerate' id='x1-3011x1'>\({\mathbf V}=\{\}\) /* empty sequence */.
     </li>
<li class='enumerate' id='x1-3013x2'>
     <!-- l. 40 --><p class='noindent'>for \({\mathbf O}_i\in {\mathbf O}\):
         </p><ol class='enumerate2'>
<li class='enumerate' id='x1-3015x1'>\({\mathbf V}_i\leftarrow \text {2D-T}^{-S}({\mathbf O}_i)\) /* 2D synthesis spatial transform */.</li></ol>
     </li></ol>
   <figure class='figure'> 

                                                                  

                                                                  
                                                                  

                                                                  
<!-- l. 49 --><p class='noindent' id='-decomposition-generated-by-levels-s-ddwt-and-the-xdct-block-size-is-equal-to-s-'><div style='text-align:center;'> <img src='graphics/forward_MDWT.svg' /> </div>  <a id='x1-3016r1'></a>
<a id='x1-3017'></a>
</p>
<figcaption class='caption'><span class='id'>Figure 1: </span><span class='content'>Decomposition generated by 1-levels (\(S=1\)) 2D-DWT and the 2x2-DCT
(block size is equal to \(S+1\)).                                              </span></figcaption><!-- tex4ht:label?: x1-3016r1  -->
                                                                  

                                                                  
   </figure>
   <h3 class='sectionHead' id='-the-ipp-mrvc-encoder-'><span class='titlemark'>2   </span> <a id='x1-40002'></a>Hybrid video coding</h3>
   <figure class='figure'> 

                                                                  

                                                                  
                                                                  

                                                                  
<!-- l. 57 --><p class='noindent' id='-hybrid-video-coding-'><div style='text-align:center;'> <img src='graphics/hybrid_coding.svg' /> </div>  <a id='x1-4001r2'></a>
<a id='x1-4002'></a>
</p>
<figcaption class='caption'><span class='id'>Figure 2: </span><span class='content'>Hybrid video coding.
</span></figcaption><!-- tex4ht:label?: x1-4001r2  -->
                                                                  

                                                                  
   </figure>
<!-- l. 62 --><p class='indent'>   See the Fig. <a href='#x1-4001r2'>2<!-- tex4ht:ref: fig:hybrid_coding  --></a>.
</p>
     <ul class='itemize1'>
     <li class='itemize'>Used in all the video compression standards.
     </li>
     <li class='itemize'>Intracoded images are transformed (\(T\)) and quantized (\(Q\)).
     </li>
     <li class='itemize'>Intercoded images are also motion compensated (\(P\)).
     </li>
     <li class='itemize'>The encoder incorporates a decoder to avoid the drift error.</li></ul>
   <h4 class='subsectionHead' id='motion-compensation-prediction-p-video-coding'><span class='titlemark'>2.1   </span> <a id='x1-50002.1'></a>Motion compensation prediction (P) video coding</h4>
<!-- l. 78 --><p class='noindent'>III... video coding is fast and good for video edition, but the compression ratios are
poor because the temporal correlation is not removed. IPP... video coding (also called
inter-coding) uses MC (Motion Compensation) to exploit the temporal correlation
in frame sequences. IPP... encoders split the sequence into <a href='https://en.wikipedia.org/wiki/Group_of_pictures'>GOPs (Groups
of Frames)</a>, where the first frame of each GOP is a I-type frame, and the
rest of frames of the GOP are P-type. P-frames are motion-compensated
frames and usually the reference frame is the previous one, in the temporal
order.
</p><!-- l. 89 --><p class='noindent'>
</p>
   <h4 class='subsectionHead' id='types-of-macroblocks'><span class='titlemark'>2.2   </span> <a id='x1-60002.2'></a>Types of macro-blocks</h4>
<!-- l. 90 --><p class='noindent'>A P-type frame is created in two steps:
     </p><ol class='enumerate1'>
<li class='enumerate' id='x1-6002x1'>The frame is divided into non-overlapping blocks (typically of \(16\times 16\) pixels),
     that in this context we will call, macro-blocks.
     </li>
<li class='enumerate' id='x1-6004x2'>A prediction frame is generated using a field of motion vectors (one vector
     per matro-block) and a reference.
                                                                  

                                                                  
     </li>
<li class='enumerate' id='x1-6006x3'>The prediction frame is substracted to the predicted frame, generating a
     prediction-error frame.</li></ol>
<!-- l. 100 --><p class='noindent'>In general, most of the time this procedure success in temporally decorrelate the frames.
However, it can also happen that when the reference(s) frame(s) is very different from
the predicted one, the prediction-error frame has a higher entropy than the original
(predicted) one.
</p><!-- l. 106 --><p class='indent'>   In order to minimize this inconvenient, most video coding standards use the
following types of macro-blocks:
     </p><ol class='enumerate1'>
<li class='enumerate' id='x1-6008x1'>I (intra): if the macro-block is not compensated.
     </li>
<li class='enumerate' id='x1-6010x2'>P (predicted): if the macro-blocks is compensated.
     </li>
<li class='enumerate' id='x1-6012x3'>B (bidirectionally predicted): if the macro-block is compensated using an
     anterior and a posterior frame.
     </li>
<li class='enumerate' id='x1-6014x4'>S  (skipped):  if  the  macro-block  is  substituted  by  reference  one  (no
     substraction is performed).</li></ol>
<!-- l. 116 --><p class='noindent'>The decision of the type of each macro-block is performed at the compressor, and this
information is signaled in the code-stream. An visual example of the decision of the
type of the macro-blocks is shown in the Figure <a href='#x1-10001r6'>6<!-- tex4ht:ref: fig:macroblocks  --></a>.
</p>
   <figure class='figure'> 

                                                                  

                                                                  
                                                                  

                                                                  
<!-- l. 123 --><p class='noindent' id='-types-of-macroblocks-'><div style='text-align:center;'> <img src='graphics/macroblocks.svg' /> </div>  <a id='x1-6015r3'></a>
<a id='x1-6016'></a>
</p>
<figcaption class='caption'><span class='id'>Figure 3: </span><span class='content'>Types of macro-blocks.                                      </span></figcaption><!-- tex4ht:label?: x1-6015r2  -->
                                                                  

                                                                  
   </figure>
   <h4 class='subsectionHead' id='macroblock-type-decision'><span class='titlemark'>2.3   </span> <a id='x1-70002.3'></a>Macro-block type decision</h4>
<!-- l. 129 --><p class='noindent'>Ideally, the type (also called, mode) of the macro-blocks is decided considering all the
posibilities (I, P, B, or S) and selecting the most beneficial one from a RD perpective,
i.e., selecting the alternative that minimizes the RD cost. This RDO is performed at
the macro-block level.
</p><!-- l. 135 --><p class='noindent'>
</p>
   <h4 class='subsectionHead' id='codec'><span class='titlemark'>2.4   </span> <a id='x1-80002.4'></a>Codec</h4>
   <figure class='figure'> 

                                                                  

                                                                  
                                                                  

                                                                  
<!-- l. 138 --><p class='noindent' id='-a-interintra-video-codec-'><div style='text-align:center;'> <img src='graphics/codec.svg' /> </div>  <a id='x1-8001r4'></a>
<a id='x1-8002'></a>
</p>
<figcaption class='caption'><span class='id'>Figure 4: </span><span class='content'>A inter/intra video codec.                                    </span></figcaption><!-- tex4ht:label?: x1-8001r2  -->
                                                                  

                                                                  
   </figure>
<!-- l. 143 --><p class='indent'>   The Figure <a href='#x1-9001r5'>5<!-- tex4ht:ref: fig:IPP_codec  --></a> describes an inter/intra video codec, that corresponds
to:
</p><!-- l. 149 --><p class='indent'>   \begin {equation}  {\mathbf W}_k = \text {color-T}({\mathbf V}_k), \tag {a}  \end {equation}

</p><!-- l. 154 --><p class='indent'>   \begin {equation}  {\mathbf W}_{k-1} = Z^{-1}({\mathbf W}, k-1), \tag {b}  \end {equation}
and by definition, \(Z^{-1}({\mathbf W}, -1) = {\mathbf 0}\),
</p><!-- l. 160 --><p class='indent'>   \begin {equation}  \overset {(k-1)\rightarrow k}{\mathbf M} = \text {ME}({\mathbf W}_{k-1}, {\mathbf W}_k), \tag {c}  \end {equation}
where ME stands for Motion Estimation, and by definition, \(\overset {(-1)\rightarrow 0}{{\mathbf M}} = {\mathbf 0}\),
</p><!-- l. 167 --><p class='indent'>   \begin {equation}  \overset {(k-1)\rightarrow k}{\mathbf M} = \overset {(k-1)\rightarrow k}{\mathbf M} \text {(lossless~coding)}, \tag {d}  \end {equation}

</p><!-- l. 172 --><p class='indent'>   \begin {equation}  \overset {(k-1)\rightarrow k}{\mathbf M} = \overset {(k-1)\rightarrow k}{\mathbf M} \text {(lossless~decoding)}, \tag {e}  \end {equation}

</p><!-- l. 177 --><p class='indent'>   \begin {equation}  {\mathbf E}_k = {\mathbf W}_k - \overset {\wedge }{{\mathbf W}}_k, \tag {f}  \end {equation}
where the symbol \(-\) represents to the pixel-wise substraction,
</p><!-- l. 183 --><p class='indent'>   \begin {equation}  \overset {\sim }{{\mathbf E}_k} = \text {QE}({\mathbf E}_k), \tag {g}  \end {equation}
where QE\((\cdot )\) represents the lossy compression of the prediction error texture
data,
</p><!-- l. 190 --><p class='indent'>   \begin {equation}  \overset {\sim }{\mathbf E}_k = \text {DQ}^{-1}(\overset {\sim }{\mathbf E}_k), \tag {h}  \end {equation}
where DQ\(^{-1}(\cdot )\) represents the decompression of the prediction error texture data,
</p><!-- l. 197 --><p class='indent'>   \begin {equation}  \overset {\sim }{\mathbf W}_k \leftarrow \overset {\sim }{\mathbf E}_k + \overset {\wedge }{\mathbf W}_k, \tag {i}  \end {equation}
and notice that if \(\overset {\wedge }{\mathbf W}_k = {\mathbf 0}\), then \(\overset {\sim }{\mathbf E}_k = \overset {\sim }{\mathbf W}_k\),
</p><!-- l. 204 --><p class='indent'>   \begin {equation}  \overset {\wedge }{\mathbf W}_k = \text {P}(\overset {\sim }{\mathbf W}_{k-1}, \overset {(k-1)\rightarrow k}{\mathbf M}), \tag {j}  \end {equation}
where P\((\cdot ,\cdot )\) is a motion compensated predictor, and
</p><!-- l. 210 --><p class='indent'>   \begin {equation}  \overset {\wedge }{\mathbf W}_{k-1} = Z^{-1}(\overset {\wedge }{\mathbf W}, k-1), \tag {k}  \end {equation}
where by definition, \(Z^{-1}(\overset {\wedge }{\mathbf W}, -1) = 0\), and
</p><!-- l. 216 --><p class='indent'>   \begin {equation}  {\mathbf V} = \text {color-T}^{-1}({\mathbf W}_k), \tag {l}  \end {equation}
is the inverse color transform.
</p><!-- l. 219 --><p class='indent'>   Notice that if \(\overset {\wedge }{{\mathbf W}}_k\) is similar to \({\mathbf W}_k\), then \({\mathbf E}_k\) will be approximately zero, and therefore,
easely compressed. Another interesting aspect to highlight is that the encoder
replicates de decoder in order to use the reconstructed images as reference and avoid
the drift error.
</p>
   <h4 class='subsectionHead' id='a-block-diagram-of-the-step-codec'><span class='titlemark'>2.5   </span> <a id='x1-90002.5'></a>A block diagram of the step codec</h4>
<!-- l. 227 --><p class='noindent'>It’s time to test the performance of the ME/MC process previously described, in the
image domain. We encode a sequence of frames \(\{W_k\}\) using the pattern IPP... which means
that the first frame will be intra-coded (I-type frame) and the rest of frames of the
GOF (Group Of (<a href='https://en.wikipedia.org/wiki/Group_of_pictures'>Frames</a>) will be predicted-coded (P-type frame), respect to the
previous one.
</p>
   <figure class='figure'> 

                                                                  

                                                                  
                                                                  

                                                                  
<!-- l. 237 --><p class='noindent' id='-a-simple-ipp-image-codec-'><div style='text-align:center;'> <img src='graphics/codec2.svg' /> </div>  <a id='x1-9001r5'></a>
<a id='x1-9002'></a>
</p>
<figcaption class='caption'><span class='id'>Figure 5: </span><span class='content'>A simple IPP... image codec.                                 </span></figcaption><!-- tex4ht:label?: x1-9001r2  -->
                                                                  

                                                                  
   </figure>
<!-- l. 242 --><p class='indent'>   The IPP... coding can be done by the codec shown in the Fig. <a href='#x1-9001r5'>5<!-- tex4ht:ref: fig:IPP_codec  --></a>, where:
</p><!-- l. 258 --><p class='indent'>   \begin {equation}  V_k \leftarrow \text {C}(W_k) = \begin {bmatrix} \frac {1}{4} &amp; \frac {1}{2} &amp; \frac {1}{4} \\ \frac {1}{2} &amp; 0 &amp; -\frac {1}{2} \\ -\frac {1}{4} &amp; \frac {1}{2} &amp; -\frac {1}{4} \end {bmatrix} \begin {bmatrix} W_k.\text {R} \\ W_k.\text {G} \\ W_k.\text {B} \end {bmatrix} , \tag {a}  \end {equation}

</p><!-- l. 263 --><p class='indent'>   \begin {equation}  Z^{-1}(V_k) = V_{k-1}, \tag {b}  \end {equation}
and by definition, \(Z^{-1}(V_{-1}) = 0\),
</p><!-- l. 269 --><p class='indent'>   \begin {equation}  \overset {k\rightarrow k-1}{V} \leftarrow \text {M}(V_k, V_{k-1}), \tag {c}  \end {equation}
where M stands for Motion Estimation, and by definition, \(\overset {0\rightarrow (-1)}{V}=0\),
</p><!-- l. 276 --><p class='indent'>   \begin {equation}  \overset {\sim }{\overset {k\rightarrow k-1}{V}} \leftarrow \text {E}_{\overset {\rightarrow }{V}}(\overset {k\rightarrow k-1}{V}), \tag {d}  \end {equation}
where E\(_{\overset {\rightarrow }{V}}(\cdot )\) represents the lossy compression of the motion data,
</p><!-- l. 283 --><p class='indent'>   \begin {equation}  \overset {\sim }{\overset {k\rightarrow k-1}{V}} \leftarrow \text {D}_{\overset {\rightarrow }{V}}(\overset {\sim }{\overset {k\rightarrow k-1}{V}}), \tag {e}  \end {equation}
where D\(_{\overset {\rightarrow }{V}}(\cdot )\) represents the decompression of the motion data,
</p><!-- l. 290 --><p class='indent'>   \begin {equation}  E_k \leftarrow V_k - \overset {\wedge }{{V}}_k, \tag {f}  \end {equation}
where the symbol \(-\) represents to the pixel-wise substraction,
</p><!-- l. 296 --><p class='indent'>   \begin {equation}  \overset {\sim }{E_k} \leftarrow \text {E}_{E}(E_k), \tag {g}  \end {equation}
where E\(_{E}(\cdot )\) represents the lossy compression of the prediction error texture
data,
</p><!-- l. 303 --><p class='indent'>   \begin {equation}  \overset {\sim }{E}_k \leftarrow \text {D}_{E}(\overset {\sim }{E}_k), \tag {h}  \end {equation}
where D\(_{E}(\cdot )\) represents the decompression of the prediction error texture data,
</p><!-- l. 310 --><p class='indent'>   \begin {equation}  \overset {\sim }{V}_k \leftarrow \overset {\sim }{E}_k + \overset {\wedge }{V}_k, \tag {i}  \end {equation}
and notice that if \(\overset {\wedge }{V}_k=0\), then \(\overset {\sim }{E}_k = \overset {\sim }{V}_k\),
</p><!-- l. 317 --><p class='indent'>   \begin {equation}  \overset {\wedge }{V}_k \leftarrow \text {P}(\overset {\sim }{\overset {k\rightarrow k-1}{V}}, \overset {\sim }{V}_{k-1}), \tag {j}  \end {equation}
where P\((\cdot ,\cdot )\) is a motion compensated predictor.
</p><!-- l. 320 --><p class='indent'>   Notice that if \(\overset {\wedge }{{V}}_k\) is similar to \(V_k\), then \(E_k\) will be approximately zero, and therefore,
easely compressed. Another interesting aspect to highlight is that the encoder
replicates de decoder in order to use the reconstructed images as reference and avoid
the drift error.
</p>
   <h3 class='sectionHead' id='blockbased-mc-motion-compensation-raotechniques'><span class='titlemark'>3   </span> <a id='x1-100003'></a>Block-based MC (Motion Compensation) <span class='cite'>[<a href='#Xrao1996techniques'>2</a>]</span></h3>
     <ul class='itemize1'>
     <li class='itemize'>Usually, only performed by the encoder (compress one. decompress many).
     </li>
     <li class='itemize'>MC removes temporal redundancy. A <span class='ecti-1000'>predicted image </span>can be encoded as
     the difference between it and another image called <span class='ecti-1000'>prediction image </span>which
     is a motion compensated projection of one or more images named <span class='ecti-1000'>reference
     </span><span class='ecti-1000'>images</span>. ME tries to generate <span class='ecti-1000'>residue images </span>as close as possible to the
     null images.
     </li>
     <li class='itemize'>For  example,  in  the  MPEG-1  standard,  the  reference  image/s  is/are
     divided in blocks of \(16\times 16\) pixels called <span class='ecti-1000'>macroblocks</span>.
     </li>
     <li class='itemize'>Each reference block is searched in the predicted image and the best match
     is indicated by mean of a <span class='ecti-1000'>motion vector</span>.
     </li>
     <li class='itemize'>
     <!-- l. 347 --><p class='noindent'>Depending on the success of the search and the number of reference images,
     the macroblocks are classified into:
</p>
         <ul class='itemize2'>
         <li class='itemize'><span class='ecbx-1000'>I (intra)</span>: When the compression of residue block generates more
         bits than the original (predicted) one.
         </li>
         <li class='itemize'><span class='ecbx-1000'>P (predicted)</span>: When it is better to compress the residue block and
         there is only one reference macroblock.
         </li>
         <li class='itemize'><span class='ecbx-1000'>B  (bidirectionally  predicted)</span>:  The  same,  but  if  we  have  two
         reference macroblocks.
         </li>
         <li class='itemize'><span class='ecbx-1000'>S (skipped)</span>: When the energy of the residue block is smaller than
         a given threshold.</li></ul>
     </li>
     <li class='itemize'>I-pictures are composed of I macroblocks, only.
     </li>
     <li class='itemize'>P-pictures do not have B macrobocks.
     </li>
     <li class='itemize'>B-pictures can have any type of macroblocks.</li></ul>
<!-- l. 372 --><p class='noindent'>See the Fig. <a href='#x1-10001r6'>6<!-- tex4ht:ref: fig:macroblocks  --></a>.
</p>
   <figure class='figure'> 

                                                                  

                                                                  
                                                                  

                                                                  
<!-- l. 375 --><p class='noindent' id='-types-of-macroblocks-1'><div style='text-align:center;'> <img src='graphics/macroblocks.svg' /> </div>  <a id='x1-10001r6'></a>
<a id='x1-10002'></a>
</p>
<figcaption class='caption'><span class='id'>Figure 6: </span><span class='content'>Types of macroblocks.
</span></figcaption><!-- tex4ht:label?: x1-10001r3  -->
                                                                  

                                                                  
   </figure>
   <h3 class='sectionHead' id='subpixel-accuracy'><span class='titlemark'>4   </span> <a id='x1-110004'></a>Sub-pixel accuracy</h3>
     <ul class='itemize1'>
     <li class='itemize'>The motion estimation can be carried out using integer pixel accuracy or
     a fractional (sub-) pixel accuracy.
     </li>
     <li class='itemize'>For example, in MPEG-1, the motion estimation can have up to 1/2 pixel
     accuracy. A bi-linear interpolator is used (see the Fig. <a href='#x1-11001r7'>7<!-- tex4ht:ref: fig:interpolation  --></a>).</li></ul>
   <figure class='figure'> 

                                                                  

                                                                  
                                                                  

                                                                  
<!-- l. 394 --><p class='noindent' id='-pixel-interpolation-'><div style='text-align:center;'> <img src='graphics/interpolation.svg' /> </div>  <a id='x1-11001r7'></a>
<a id='x1-11002'></a>
</p>
<figcaption class='caption'><span class='id'>Figure 7: </span><span class='content'>Pixel interpolation.
</span></figcaption><!-- tex4ht:label?: x1-11001r4  -->
                                                                  

                                                                  
   </figure>
   <h3 class='sectionHead' id='matching-criteria-similitude-between-macroblocks'><span class='titlemark'>5   </span> <a id='x1-120005'></a>Matching criteria (similitude between macroblocks)</h3>
     <ul class='itemize1'>
     <li class='itemize'>
     <!-- l. 402 --><p class='noindent'>Let \(a\) and \(b\) the macroblocks which we want to compare. Two main distortion
     metrics are commonly used:
</p>
         <ul class='itemize2'>
         <li class='itemize'>
         <!-- l. 407 --><p class='noindent'><span class='ecbx-1000'>MSE (Mean Square Error)</span>:
         </p><!-- l. 411 --><p class='noindent'>\begin {equation}  \frac {1}{16\times 16}\sum _{i=1}^{16}\sum _{j=1}^{16}(a_{ij}-b_{ij})^2  \end {equation}
         </p></li>
         <li class='itemize'>
         <!-- l. 413 --><p class='noindent'><span class='ecbx-1000'>MAE (Mean Absolute Error)</span>:
         </p><!-- l. 417 --><p class='noindent'>\begin {equation}  \frac {1}{16\times 16}\sum _{i=1}^{16}\sum _{j=1}^{16}|a_{ij}-b_{ij}|  \end {equation}
         </p></li></ul>
     </li>
     <li class='itemize'>These similitude measures are used only by MPEG compressors. Therefore, any
     other one with similar effects (such as the error variance or the error entropy)
     could be used also.
     </li>
     <li class='itemize'>
     <!-- l. 424 --><p class='noindent'>Other less common distortion metrics that can work are:
</p>
         <ul class='itemize2'>
         <li class='itemize'>
         <!-- l. 428 --><p class='noindent'><span class='ecbx-1000'>EE (Error </span><a href='https://en.wikipedia.org/wiki/Entropy_(information_theory)'><span class='ecbx-1000'>Entropy</span></a>:
         </p><!-- l. 433 --><p class='noindent'>\begin {equation}  -\frac {1}{16\times 16}\sum _{i=1}^{16}\sum _{j=1}^{16}\log _2(a_{ij}-b_{ij})p(a_{ij}-b_{ij})  \end {equation}
         </p></li></ul>
     </li></ul>
                                                                  

                                                                  
<!-- l. 437 --><p class='noindent'>
</p>
   <h3 class='sectionHead' id='searching-strategies'><span class='titlemark'>6   </span> <a id='x1-130006'></a>Searching strategies</h3>
     <ul class='itemize1'>
     <li class='itemize'>
     <!-- l. 440 --><p class='noindent'>Only performed by the compressor.
</p>
         <ul class='itemize2'>
         <li class='itemize'><span class='ecbx-1000'>Full  search</span>:  All  the  possibilities  are  checked  (see  the  Fig. <a href='#x1-13001r8'>8<!-- tex4ht:ref: fig:full_search  --></a>).
         Advantage: the best compression. Disadvantage: CPU killer.</li></ul>
     <figure class='figure' id='-the-full-search-scheme-'> 
<div style='text-align:center;'> <img src='graphics/full_search.svg' /> </div>   <a id='x1-13001r8'></a>
<a id='x1-13002'></a>
<figcaption class='caption'><span class='id'>Figure 8: </span><span class='content'>The full search scheme.
</span></figcaption><!-- tex4ht:label?: x1-13001r6  -->
     </figure>
         <ul class='itemize2'>
         <li class='itemize'>** Logaritmic search**: It is a version of the full search algorithm
         where the macro-blocks and the search area are sub-sampled. After
         finding the best coincidence, the resolution is increased in a power
         of 2 and the previous match is refined in a search area of \(\pm 1\), until the
         maximal resolution (even using subpixel accuracy) is reached.</li></ul>
         <ul class='itemize2'>
         <li class='itemize'><span class='ecbx-1000'>Telescopic search</span>: Any of the previously described techniques can
         be speeded up if the searching area is reduced. This can be done
         supposing that the motion vector of the same macro-block in two
         consecutive images is similar.</li></ul>
     </li></ul>
                                                                  

                                                                  
<!-- l. 474 --><p class='noindent'>
</p>
   <h3 class='sectionHead' id='the-gop-group-of-pictures-concept'><span class='titlemark'>7   </span> <a id='x1-140007'></a>The GOP (Group Of Pictures) concept</h3>
     <ul class='itemize1'>
     <li class='itemize'>The temporal redundancy is exploited by blocks of images called GOPs.
     This means that a GOP can be decoded independently of the rest of GOPs
     (see the Fig. <a href='#x1-14001r9'>9<!-- tex4ht:ref: fig:GOPs  --></a>).</li></ul>
   <figure class='figure'> 

                                                                  

                                                                  
                                                                  

                                                                  
<!-- l. 484 --><p class='noindent' id='-a-gop-'><div style='text-align:center;'> <img src='graphics/GOPs.svg' /> </div>  <a id='x1-14001r9'></a>
<a id='x1-14002'></a>
</p>
<figcaption class='caption'><span class='id'>Figure 9: </span><span class='content'>A GOP.
</span></figcaption><!-- tex4ht:label?: x1-14001r7  -->
                                                                  

                                                                  
   </figure>
   <h3 class='sectionHead' id='mctf-motion-compensated-temporal-filtering'><span class='titlemark'>8   </span> <a id='x1-150008'></a>MCTF (Motion Compensated Temporal Filtering)</h3>
     <ul class='itemize1'>
     <li class='itemize'>This is a DWT where the input samples are the original video images and
     the output is a sequence of residue images.</li></ul>
<!-- l. 497 --><p class='noindent'>
</p>
   <h3 class='sectionHead' id='-spiralsearch-me-motion-estimation'><span class='titlemark'>9   </span> <a id='x1-160009'></a>\(\pm \) 1-spiral-search ME (Motion Estimation)</h3>
   <figure class='figure'> 

                                                                  

                                                                  
                                                                  

                                                                  
<!-- l. 499 --><p class='noindent' id='-spiral-search-'><div style='text-align:center;'> <img src='graphics/spiral_search.svg' /> </div>  <a id='x1-16001r10'></a>
<a id='x1-16002'></a>
</p>
<figcaption class='caption'><span class='id'>Figure 10: </span><span class='content'>Spiral search.
</span></figcaption><!-- tex4ht:label?: x1-16001r9  -->
                                                                  

                                                                  
   </figure>
   <h3 class='sectionHead' id='linear-frame-interpolation-using-blockbased-motion-compensation'><span class='titlemark'>10   </span> <a id='x1-1700010'></a>Linear frame interpolation using block-based motion compensation</h3>
   <figure class='figure'> 

                                                                  

                                                                  
                                                                  

                                                                  
<!-- l. 507 --><p class='noindent' id='-frame-interpolation-'><div style='text-align:center;'> <img src='graphics/frame_interpolation.svg' /> </div>  <a id='x1-17001r11'></a>
<a id='x1-17002'></a>
</p>
<figcaption class='caption'><span class='id'>Figure 11: </span><span class='content'>Frame interpolation.
</span></figcaption><!-- tex4ht:label?: x1-17001r10  -->
                                                                  

                                                                  
   </figure>
   <h4 class='likesubsectionHead' id='input'><a id='x1-1800010'></a>Input</h4>
     <ul class='itemize1'>
     <li class='itemize'>\(R\): square search area, in pixels.
     </li>
     <li class='itemize'>\(B\): square block size, in pixels.
     </li>
     <li class='itemize'>\(O\): border size, in pixels.
     </li>
     <li class='itemize'>\(s_i\), \(s_j\) and \(s_k\) three chronologically ordered, equidistant frames, with resolution
     \(X\times Y\).
     </li>
     <li class='itemize'>\(A\): \(\frac {1}{2^A}\) subpixel accuracy.</li></ul>
<!-- l. 528 --><p class='noindent'>
</p>
   <h4 class='likesubsectionHead' id='output'><a id='x1-1900010'></a>Output</h4>
     <ul class='itemize1'>
     <li class='itemize'>\(\hat {s}_j\): a prediction for frame \(s_j\).
     </li>
     <li class='itemize'>\(m\): a matrix with \(\lceil X/B\rceil \times \lceil Y/B\rceil \) bidirectional motion vectors.
     </li>
     <li class='itemize'>\(e\):  a  matrix  with  \(\lceil X/B\rceil \times \lceil Y/B\rceil \)  bidirectional  Root  Mean  Square  matching  Wrrors
     (RMSE).</li></ul>
<!-- l. 541 --><p class='noindent'>
</p>
   <h4 class='likesubsectionHead' id='algorithm'><a id='x1-2000010'></a>Algorithm</h4>
<!-- l. 542 --><p class='noindent'>
                                                                  

                                                                  
     </p><ol class='enumerate1'>
<li class='enumerate' id='x1-20002x1'>Compute the DWT\(^l\), where \(l=\lfloor \log _2(R)\rfloor -1\) levels, of the predicted frame \(s_j\) and the two
     reference frames \(s_i\) and \(s_k\). <a href='https://vicente-gonzalez-ruiz.github.io/video_compression/graphics/frame_interpolation_step_1.svg'>Example</a>.
     </li>
<li class='enumerate' id='x1-20004x2'>\(LL^l(m)\leftarrow 0\), or any other precomputed values (for example, from a previous ME in
     neighbor frames). <a href='https://vicente-gonzalez-ruiz.github.io/video_compression/graphics/frame_interpolation_step_2.svg'>Example</a>.
     </li>
<li class='enumerate' id='x1-20006x3'>Divide the subband \(LL^l(s_j)\) into blocks of size \(B\times B\) pixels, and \(\pm 1\)-spiral-search them
     in the subbands \(LL^l(s_i)\) and \(LL^l(s_k)\), calculating a low-resolution \(LL^l(m)=\{LL^l(\overleftarrow {m}), LL^l(\overrightarrow {m})\}\) bi-directional motion
     vector field. <a href='https://vicente-gonzalez-ruiz.github.io/video_compression/graphics/frame_interpolation_step_3A.svg'>Example</a>. <a href='https://vicente-gonzalez-ruiz.github.io/video_compression/graphics/frame_interpolation_step_3A_bis.svg'>Example</a>.
     </li>
<li class='enumerate' id='x1-20008x4'>
     <!-- l. 565 --><p class='noindent'>While \(l&gt;0\):
     </p><!-- l. 567 --><p class='noindent'>
         </p><ol class='enumerate2'>
<li class='enumerate' id='x1-20010x1'>Synthesize  \(LL^{l-1}(m)\),  \(LL^{l-1}(s_j)\),  \(LL^{l-1}(s_i)\)  and  \(LL^{l-1}(s_k)\),  by  computing  the  1-level  DWT\(^{-1}\).  <a href='https://vicente-gonzalez-ruiz.github.io/video_compression/graphics/frame_interpolation_step_4A.svg'>Example</a>.
         <a href='https://vicente-gonzalez-ruiz.github.io/video_compression/graphics/frame_interpolation_step_4A_bis.svg'>Example</a>
         </li>
<li class='enumerate' id='x1-20012x2'>\(LL^{l-1}(M)\leftarrow LL^{l-1}(M)\times 2\). <a href='https://vicente-gonzalez-ruiz.github.io/video_compression/graphics/frame_interpolation_step_4B.svg'>Example</a>.
         </li>
<li class='enumerate' id='x1-20014x3'>Refine \(LL^{l-1}(m)\) using \(\pm 1\)-spiral-search. <a href='https://vicente-gonzalez-ruiz.github.io/video_compression/graphics/frame_interpolation_step_4C.svg'>Example</a>.
         </li>
<li class='enumerate' id='x1-20016x4'>\(l\leftarrow l-1\). (When \(l=0\), the motion vectors field \(m\) has the structure:)
</li></ol>
     <!-- l. 589 --><p class='noindent'><a href='https://vicente-gonzalez-ruiz.github.io/video_compression/graphics/motion_vectors.svg'>Example</a>.
     </p></li>
                                                                  

                                                                  
<li class='enumerate' id='x1-20018x5'>
     <!-- l. 592 --><p class='noindent'>While \(l&lt;A\) (in the first iteration, \(l=0\), and \(LL^0(M):=M\)):
     </p><!-- l. 594 --><p class='noindent'>
         </p><ol class='enumerate2'>
<li class='enumerate' id='x1-20020x1'>\(l\leftarrow l+1\).
         </li>
<li class='enumerate' id='x1-20022x2'>
         <!-- l. 599 --><p class='noindent'>Synthesize  \(LL^{-l}(s_j)\),  \(LL^{-l}(s_i)\)  and  \(LL^{-l}(s_k)\),  computing  the  1-level  DWT\(^{-1}\)  (high-frequency
         subbands  are  \(0\)).  This  performs  a  zoom-in  in  these  frames  using
         \(1/2\)-subpixel accuracy.
         </p><!-- l. 604 --><p class='noindent'><a href='https://vicente-gonzalez-ruiz.github.io/video_compression/graphics/frame_interpolation_step_5B.svg'>Example</a>.
         </p></li>
<li class='enumerate' id='x1-20024x3'>
         <!-- l. 607 --><p class='noindent'>\(m\leftarrow m\times 2\).
         </p><!-- l. 609 --><p class='noindent'><a href='https://vicente-gonzalez-ruiz.github.io/video_compression/graphics/motion_vectors_by_2.svg'>Example</a>.
         </p></li>
<li class='enumerate' id='x1-20026x4'>\(B\leftarrow B\times 2\).
         </li>
<li class='enumerate' id='x1-20028x5'>Divide the subband \(LL^{-l}(s_j)\) into blocks of \(B\times B\) pixels and \(\pm 1\)-spiral-search them into
         the subbands \(LL^{-l}(s_i)\) and \(LL^{-l}(s_k)\), calculating a \(1/2^l\) sub-pixel accuracy \(m\) bi-directional
         motion vector field. <a href='https://vicente-gonzalez-ruiz.github.io/video_compression/graphics/motion_vectors_definitive.svg'>Example</a>.
         </li>
<li class='enumerate' id='x1-20030x6'>Frame prediction. For each block \(b\):
         </li>
<li class='enumerate' id='x1-20032x7'>
         <!-- l. 625 --><p class='noindent'>Compute \begin {equation}  \hat {b}\leftarrow \frac {b_i\big (\overleftarrow {e}_{\text {max}}-\overleftarrow {e}(b)\big ) + b_k\big (\overrightarrow {e}_{\text {max}}-\overrightarrow {e}(b)\big )}{\big (\overleftarrow {e}_{\text {max}}-\overleftarrow {e}(b)\big ) + \big (\overrightarrow {e}_{\text {max}}-\overrightarrow {e}(b)\big )},  \end {equation}
         </p><!-- l. 630 --><p class='noindent'>where \(\overleftarrow {e}(b)\) is the (minimum) distortion of the best backward matching for
         block \(b\), \(\overrightarrow {e}(b)\) the (minimum) distortion of the best forward matching for block \(b\), \(\overleftarrow {e}_{\text {max}}=\overrightarrow {e}_{\text {max}}\)
         are the backward and forward maximum matching distortions, \(b_i\) is the
         (backward) block found (as the most similar to \(b\)) in frame \(s_i\) and \(b_k\) is the
                                                                  

                                                                  
         (forward) block found in frame \(s_k\). Notice that, if \(\overleftarrow {e}(b)=\overrightarrow {e}(b)\), then the prediction is
         \begin {equation}  \hat {b} = \frac {b_i + b_k}{2},  \end {equation}
         and if \(\overleftarrow {e}(b)=0\), \begin {equation}  \hat {b} = b_k,  \end {equation}
         and viceversa.</p></li></ol>
     </li></ol>
<!-- l. 651 --><p class='noindent'>
</p>
   <h4 class='likesubsectionHead' id='lab'><a id='x1-2100010'></a>Lab</h4>
<!-- l. 652 --><p class='noindent'>Implement  the  Section  <a href='#linear-frame-interpolation-using-blockbased-motion-compensation'>10<!-- tex4ht:ref: sec:linear_frame_interpolation  --></a>  (work  on
<a class='url' href='https://github.com/Sistemas-Multimedia/MCDWT/blob/master/transform/mc/block/interpolate.py'><span class='ectt-1000'>https://github.com/Sistemas-Multimedia/MCDWT/blob/master/transform/mc/block/interpolate.py</span></a>).
Use <a class='url' href='https://github.com/Sistemas-Multimedia/MCDWT/blob/master/mcdwt/mc/block/interpolate.py'><span class='ectt-1000'>https://github.com/Sistemas-Multimedia/MCDWT/blob/master/mcdwt/mc/block/interpolate.py</span></a> and
<a class='url' href='https://github.com/vicente-gonzalez-ruiz/MCTF-video-coding/blob/master/src/motion\_estimate.cpp'><span class='ectt-1000'>https://github.com/vicente-gonzalez-ruiz/MCTF-video-coding/blob/master/src/motion\_estimate.cpp</span></a>
as reference.
</p><!-- l. 660 --><p class='noindent'>
</p>
   <h4 class='likesubsectionHead' id='lab1'><a id='x1-2200010'></a>Lab</h4>
<!-- l. 661 --><p class='noindent'>Compare the performance of the proposed matching strategies (MSE, MAE and EE)
in the Section <a href='#linear-frame-interpolation-using-blockbased-motion-compensation'>10<!-- tex4ht:ref: sec:linear_frame_interpolation  --></a>, by computing the variance of the prediction error between the
original frame (\(s_j\)) and the prediction frame (\(\hat {s}_j\)).
</p><!-- l. 666 --><p class='noindent'>
</p>
   <h4 class='likesubsectionHead' id='lab2'><a id='x1-2300010'></a>Lab</h4>
<!-- l. 667 --><p class='noindent'>Test different DWT filters in the Section <a href='#linear-frame-interpolation-using-blockbased-motion-compensation'>10<!-- tex4ht:ref: sec:linear_frame_interpolation  --></a> and compare their performance.
Compute the prediction error between the original frame (\(s_j\)) and the prediction frame
(\(\hat {s}_j\)). Measure the dependency between this performance and the distance between
frames (\(i\), \(j\), and \(k\) indexes).
</p><!-- l. 674 --><p class='noindent'>
</p>
   <h4 class='likesubsectionHead' id='lab3'><a id='x1-2400010'></a>Lab</h4>
<!-- l. 675 --><p class='noindent'>Test the use of both the luma and the chroma in Section <a href='#linear-frame-interpolation-using-blockbased-motion-compensation'>10<!-- tex4ht:ref: sec:linear_frame_interpolation  --></a>, and measure the
performance of each option (only luma vs. all components), by computing the
prediction error between the original frame (\(s_j\)) and the prediction frame (\(\hat {s}_j\)). Measure
the dependency of the results with the distance between frames (\(i\), \(j\), and \(k\)
indexes).
                                                                  

                                                                  
</p><!-- l. 682 --><p class='noindent'>
</p>
   <h4 class='likesubsectionHead' id='lab4'><a id='x1-2500010'></a>Lab</h4>
<!-- l. 683 --><p class='noindent'>Analyze the impact of the \(R\) (search range) parameter in the Section <a href='#linear-frame-interpolation-using-blockbased-motion-compensation'>10<!-- tex4ht:ref: sec:linear_frame_interpolation  --></a>. Compute the
prediction error between the original frame (\(s_j\)) and the prediction frame (\(\hat {s}_j\)). Study the
impact of initializing the motion vectors (Section <span class='ecbx-1000'>??</span>). Measure the dependency with
the distance between frames (\(i\), \(j\), and \(k\) indexes).
</p><!-- l. 690 --><p class='indent'>   <a href='https://nbviewer.jupyter.org/github/vicente-gonzalez-ruiz/video_coding/blob/master/search_range.ipynb'>IPython notebook</a>
</p><!-- l. 692 --><p class='noindent'>
</p>
   <h4 class='likesubsectionHead' id='lab5'><a id='x1-2600010'></a>Lab</h4>
<!-- l. 693 --><p class='noindent'>Analyze the impact of the \(O\) (overlaping) parameter in the Section <span class='ecbx-1000'>??</span>, by means of
computing the prediction error between the original frame (\(s_j\)) and the prediction
frame (\(\hat {s}_j\)). Measure the dependency with the distance between frames (\(i\), \(j\), and \(k\)
indexes).
</p><!-- l. 699 --><p class='noindent'>
</p>
   <h4 class='likesubsectionHead' id='lab6'><a id='x1-2700010'></a>Lab</h4>
<!-- l. 700 --><p class='noindent'>Analyze the impact of the \(B\) (block size) parameter in the Section <span class='ecbx-1000'>??</span>, by
computing the prediction error between the original frame (\(s_j\)) and the prediction
frame (\(\hat {s}_j\)). Compute the expected size of the motion fields using their 0-order
entropy. Measure the dependency with the distance between frames (\(i\), \(j\), and \(k\)
indexes).
</p><!-- l. 707 --><p class='noindent'>
</p>
   <h4 class='likesubsectionHead' id='lab7'><a id='x1-2800010'></a>Lab</h4>
<!-- l. 708 --><p class='noindent'>Analyze the impact of the \(A\) (subpixel accuracy) parameter in the Section <span class='ecbx-1000'>??</span>,
by computing the prediction error between the original frame (\(s_j\)) and the
prediction frame (\(\hat {s}_j\)). Compute the expected size of the motion fields using their
entropy. Measure the dependency with the distance between frames (\(i\), \(j\), and \(k\)
indexes).
</p><!-- l. 715 --><p class='indent'>   <a href='https://nbviewer.jupyter.org/github/vicente-gonzalez-ruiz/video_coding/blob/master/subpixel_accuracy.ipynb'>IPython notebook</a>
</p><!-- l. 717 --><p class='noindent'>
</p>
   <h4 class='likesubsectionHead' id='lab8'><a id='x1-2900010'></a>Lab</h4>
<!-- l. 718 --><p class='noindent'>Compare the performance of the Section <span class='ecbx-1000'>??</span> when it holds that
                                                                  

                                                                  
</p><!-- l. 723 --><p class='indent'>   \begin {equation}  \hat {b} = \frac {b_i + b_k}{2},  \end {equation}
</p><!-- l. 725 --><p class='indent'>   for all blocks.
</p><!-- l. 727 --><p class='noindent'>
</p>
   <h3 class='sectionHead' id='mcdwt-hybrid-coding-alternatives'><span class='titlemark'>11   </span> <a id='x1-3000011'></a>MC/DWT hybrid coding alternatives</h3>
     <ul class='itemize1'>
     <li class='itemize'><span class='ecbx-1000'>t+2d</span>: The sequence of images is decorrelated first along the time (t) and
     the residue images are compressed, exploiting the remaining spatial (2d)
     redundancy. Examples: MPEG* and H.26* codecs (except H.264/SVC).
     </li>
     <li class='itemize'><span class='ecbx-1000'>2d+t</span>: The spatial (2d) redudancy is explited first (using typically the
     DWT) and after that, the coefficients are decorrelated along the time (t).
     For now, this has only been an experimental setup because most DWT
     transformed domains are not invariant to the displacement, and therefore,
     ME/MC can not be directly applied.
     </li>
     <li class='itemize'><span class='ecbx-1000'>2d+t+2d</span>:  The  fist  step  creates  a  Laplacian  Pyramid  (2d),  which
     is  invariant  to  the  displacement.  Next,  each  level  of  the  pyramid
     is  decorrelated  along  the  time  (t)  and  finally,  the  remaining  spatial
     redundancy is removed (2d). The Fig <a href='#x1-30001r12'>12<!-- tex4ht:ref: fig:H264-S-SVC  --></a> show an example for H.264/SVC.</li></ul>
   <figure class='figure'> 

                                                                  

                                                                  
                                                                  

                                                                  
<!-- l. 749 --><p class='noindent' id='-svc-scheme-in-h-'><div style='text-align:center;'> <img src='graphics/H264-S-SVC.svg' /> </div>  <a id='x1-30001r12'></a>
<a id='x1-30002'></a>
</p>
<figcaption class='caption'><span class='id'>Figure 12: </span><span class='content'>SVC scheme in H.264.
</span></figcaption><!-- tex4ht:label?: x1-30001r11  -->
                                                                  

                                                                  
   </figure>
   <h3 class='sectionHead' id='deblocking-filtering'><span class='titlemark'>12   </span> <a id='x1-3100012'></a>Deblocking filtering</h3>
     <ul class='itemize1'>
     <li class='itemize'>If   any   other   block-overlaping   techniques   have   not   been   applied,
     block-based video encoders improve their performance if a deblocking filter
     in used to create the quantized prediction predictions (see the Fig. <a href='#x1-31001r13'>13<!-- tex4ht:ref: fig:350px-Deblock1  --></a>).</li></ul>
   <figure class='figure'> 

                                                                  

                                                                  
                                                                  

                                                                  
<!-- l. 765 --><p class='noindent' id='-deblocking-filetering-effect-'><div style='text-align:center;'> <img src='graphics/350px-Deblock1.jpg' /> </div>  800 <a id='x1-31001r13'></a>
<a id='x1-31002'></a>
</p>
<figcaption class='caption'><span class='id'>Figure 13: </span><span class='content'>Deblocking filetering effect.
</span></figcaption><!-- tex4ht:label?: x1-31001r12  -->
                                                                  

                                                                  
   </figure>
     <ul class='itemize1'>
     <li class='itemize'>The low-pass filter is applied only on the block boundaries.</li></ul>
   <h3 class='sectionHead' id='bitrate-allocation'><span class='titlemark'>13   </span> <a id='x1-3200013'></a>Bit-rate allocation</h3>
     <ul class='itemize1'>
     <li class='itemize'>
     <!-- l. 779 --><p class='noindent'>VBR: Under a constant quantization level (constant video quality), the
     number of bits that each compressed image needs depends on the image
     content (Variable Bit-Rate). In the Fig. <a href='#x1-32001r14'>14<!-- tex4ht:ref: fig:closed-loop-1_ir  --></a> there is an example. </p><figure class='figure' id='-example-of-variable-bitallocation-'> 
<div style='text-align:center;'> <img src='graphics/closed-loop-1_ir.svg' /> </div>   <a id='x1-32001r14'></a>
<a id='x1-32002'></a>
<figcaption class='caption'><span class='id'>Figure 14: </span><span class='content'>Example of variable bit-allocation.
</span></figcaption><!-- tex4ht:label?: x1-32001r13  -->
     </figure>
     </li>
     <li class='itemize'>
     <!-- l. 791 --><p class='noindent'>CBR: Using a Constant Bit-Rate strategy, all frames need the same space.
     In the Fig. <span class='ecbx-1000'>??</span> there is an example. </p><figure class='figure' id='-example-of-constant-bitallocation-'> 
<div style='text-align:center;'> <img src='graphics/CBR.svg' /> </div>   <a id='x1-32003r15'></a>
<a id='x1-32004'></a>
<figcaption class='caption'><span class='id'>Figure 15: </span><span class='content'>Example of constant bit-allocation.
</span></figcaption><!-- tex4ht:label?: x1-32003r13  -->
     </figure>
     </li></ul>
<!-- l. 802 --><p class='noindent'>
</p>
   <h3 class='sectionHead' id='video-scalability'><span class='titlemark'>14   </span> <a id='x1-3300014'></a>Video scalability</h3>
<!-- l. 803 --><p class='noindent'>
</p>
   <h4 class='subsectionHead' id='quality-scalability'><span class='titlemark'>14.1   </span> <a id='x1-3400014.1'></a>Quality scalability</h4>
   <figure class='figure'> 

                                                                  

                                                                  
                                                                  

                                                                  
<!-- l. 806 --><p class='noindent' id='-quality-scalability-'><div style='text-align:center;'> <img src='graphics/quality-scalability.svg' /> </div>  <a id='x1-34001r16'></a>
<a id='x1-34002'></a>
</p>
<figcaption class='caption'><span class='id'>Figure 16: </span><span class='content'>Quality scalability.
</span></figcaption><!-- tex4ht:label?: x1-34001r14  -->
                                                                  

                                                                  
   </figure>
<!-- l. 811 --><p class='indent'>   Véase la Fig. <a href='#x1-34001r16'>16<!-- tex4ht:ref: fig:quality-scalability  --></a>. </p>
     <ul class='itemize1'>
     <li class='itemize'>Ideal for remote visualization environments.
     </li>
     <li class='itemize'>By definition, \(V^{[0]}:=V\).</li></ul>
   <h4 class='subsectionHead' id='temporal-scalability'><span class='titlemark'>14.2   </span> <a id='x1-3500014.2'></a>Temporal scalability</h4>
   <figure class='figure'> 

                                                                  

                                                                  
                                                                  

                                                                  
<!-- l. 821 --><p class='noindent' id='-temporal-scalability-'><div style='text-align:center;'> <img src='graphics/temporal-scalability.svg' /> </div>  <a id='x1-35001r17'></a>
<a id='x1-35002'></a>
</p>
<figcaption class='caption'><span class='id'>Figure 17: </span><span class='content'>Temporal scalability.
</span></figcaption><!-- tex4ht:label?: x1-35001r14  -->
                                                                  

                                                                  
   </figure>
<!-- l. 826 --><p class='indent'>   Véase la Fig. <a href='#x1-35001r17'>17<!-- tex4ht:ref: fig:temporal-scalability  --></a>.
</p>
     <ul class='itemize1'>
     <li class='itemize'>
     <!-- l. 830 --><p class='noindent'>It holds that
     </p><!-- l. 834 --><p class='noindent'>\begin {equation}  V^{t}=\{V_{2^t i}\}=\{V_{2i}^{t-1}\},  \end {equation}
     </p><!-- l. 836 --><p class='noindent'>where \(t\) denotes the Temporal Resolution Level (TRL).
     </p></li>
     <li class='itemize'>Notice that \(V:=V^{0}\).
     </li>
     <li class='itemize'>Useful for fast random access.</li></ul>
   <h4 class='subsectionHead' id='-httpinsteecsberkeleyedu-eetsplecturesvideowaveletucbpdfspatial-scalability'><span class='titlemark'>14.3   </span> <a id='x1-3600014.3'></a><a href='http://inst.eecs.berkeley.edu/~ee290t/sp04/lectures/videowavelet_UCB1-3.pdf'>Spatial scalability</a></h4>
   <figure class='figure'> 

                                                                  

                                                                  
                                                                  

                                                                  
<!-- l. 849 --><p class='noindent' id='-spatial-scalability-'><div style='text-align:center;'> <img src='graphics/spatial-scalability.svg' /> </div>  <a id='x1-36001r18'></a>
<a id='x1-36002'></a>
</p>
<figcaption class='caption'><span class='id'>Figure 18: </span><span class='content'>Spatial scalability.
</span></figcaption><!-- tex4ht:label?: x1-36001r14  -->
                                                                  

                                                                  
   </figure>
<!-- l. 854 --><p class='indent'>   Véase la Fig. <a href='#x1-36001r18'>18<!-- tex4ht:ref: fig:spatial-scalability  --></a>.
</p>
     <ul class='itemize1'>
     <li class='itemize'>Useful for low-resolution devices.
     </li>
     <li class='itemize'>By definition, \(V_i:=V_i^{&lt;0&gt;}\) and \(V_i^{&lt;s&gt;}\) has a \(\frac {Y}{2^s}\times \frac {X}{2^s}\) resolution, where \(X\times Y\) is the resolution of \(V_i\).</li></ul>
   <h3 class='sectionHead' id='coding-in-the-transform-domain'><span class='titlemark'>15   </span> <a id='x1-3700015'></a>Coding in the Transform Domain</h3>
<!-- l. 867 --><p class='noindent'>
</p>
   <h4 class='subsectionHead' id='the-ipp-decorrelation-pattern'><span class='titlemark'>15.1   </span> <a id='x1-3800015.1'></a>The IPP... decorrelation pattern</h4>
<!-- l. 868 --><p class='noindent'>It’s time to put together all the “tools” that we have developed for encoding a
sequence of frames \(\{V_k\}\). First, the sequence will be splitted into GOFs (<a href='https://en.wikipedia.org/wiki/Group_of_pictures'>Group Of
Frames</a>), and the structure of each GOF will be IPP... <span class='cite'>[<a href='#Xle1991mpeg'>1</a>]</span>, which means that
the first frame of each GOF will be intra-coded (I-type), and the rest of
frames of the GOF will be predicted-coded (P-type), respect to the previous
one<span class='footnote-mark'><a href='#fn2x0' id='fn2x0-bk'><sup class='textsuperscript'>2</sup></a></span><a id='x1-38001f2'></a>.
Notice that in an I-type frame all the coefficients (<span class='ecti-1000'>coeffs </span>in short, remember that we
are compensating the motion in the DWT domain) will be I-type coeffs, and in a
P-type frame, the different coeffs will be I-type or P-type.
</p><!-- l. 882 --><p class='noindent'>
</p>
   <h4 class='subsectionHead' id='a-block-diagram-of-the-step-codec1'><span class='titlemark'>15.2   </span> <a id='x1-3900015.2'></a>A block diagram of the step codec</h4>
   <figure class='figure'> 

                                                                  

                                                                  
                                                                  

                                                                  
<!-- l. 886 --><p class='noindent' id='-the-ipp-mrvc-step-codec-notice-that-the-input-to-the-step-encoder-is-a-dwt-transformed-sequence-of-frames-'><div style='text-align:center;'> <img src='graphics/codec4.svg' /> </div>  <a id='x1-39001r19'></a>
<a id='x1-39002'></a>
</p>
<figcaption class='caption'><span class='id'>Figure 19: </span><span class='content'>The IPP... MRVC step codec. Notice that the input to the (step)
encoder is a DWT transformed sequence of frames.                       </span></figcaption><!-- tex4ht:label?: x1-39001r15  -->
                                                                  

                                                                  
   </figure>
<!-- l. 892 --><p class='indent'>   The MRVC IPP... step (one resolution level) codec has been described in the
Fig. <a href='#x1-39001r19'>19<!-- tex4ht:ref: fig:codec  --></a>. The equations that describe this system are:
</p><!-- l. 898 --><p class='indent'>   \begin {equation}  (V_k.L, V_k.H) \leftarrow \text {DWT}(V_k), \tag {a}  \end {equation}
where \(\leftarrow \) denotes the <a href='https://en.wikipedia.org/wiki/Assignment_(computer_science)'>assignment</a> operator, and \(V_k\) is the \(k\)-th frame of the sequence,
</p><!-- l. 906 --><p class='indent'>   \begin {equation}  [V_k.L] \leftarrow \text {DWT}^{-1}(V_k.L, 0), \tag {E.a}  \end {equation}

</p><!-- l. 911 --><p class='indent'>   \begin {equation}  Z^{-1}([V_k.L]) = [V_{k-1}.L], \tag {E.b}  \end {equation}
and by definition, \(Z^{-1}([V_{-1}.L]) = 0\),
</p><!-- l. 917 --><p class='indent'>   \begin {equation}  \overset {k\rightarrow k-1}{V} \leftarrow \text {M}([V_k.L], [V_{k-1}.L]), \tag {E.c}  \end {equation}
where M stands for Motion estimation, and by definition, \(\overset {0\rightarrow (-1)}{V}=0\),
</p><!-- l. 924 --><p class='indent'>   \begin {equation}  [\hat {V}_k.L] \leftarrow \text {P}(\overset {k\rightarrow k-1}{V}, [V_{k-1}.L]), \tag {E.d}  \end {equation}
where P stands for motion compensated Prediction,
</p><!-- l. 930 --><p class='indent'>   \begin {equation}  [E_k.L] \leftarrow [V_k.L] - [\hat {V}_k.L], \tag {E.e}  \end {equation}

</p><!-- l. 935 --><p class='indent'>   \begin {equation}  \{[M_k],[S_k]\} \leftarrow \text {EW-min}([V_k.L], [E_k.L]) \tag {E.f}  \end {equation}
where \begin {equation}  [M_k]_{i,j}=\text {min}([V_k.L]_{i,j}, [E_k.L]_{i,j}),  \end {equation}
and \([S_k]\) is a binary matrix defined by \begin {equation}  [S_k]_{i,j} = \left \{ \begin {array}{lll} 0 &amp; \text {if}~[V_k.L]_{i,j} &lt; [E_k.L]_{i,j} &amp; \text {(I-type coeff)} \\ 1 &amp; \text {otherwise} &amp; \text {(P-type coeff)}, \end {array} \right . \label {eq:matrix}  \end {equation}
(notice that \([M_k]\), that contains the element-wise minimum of both matrices, is
discarded)
</p><!-- l. 956 --><p class='indent'>   \begin {equation}  [V_k.H] \leftarrow \text {DWT}^{-1}(0, V_k.H), \tag {b}  \end {equation}

</p><!-- l. 961 --><p class='indent'>   \begin {equation}  [E_k.H] \leftarrow [V_k.H] - [\hat {V}_k.H], \tag {c}  \end {equation}
where, notice that \begin {equation}  [E_k.H]_{i,j} = \left \{ \begin {array}{ll} {[}V_k.H{]}_{i,j} &amp; \text {if}~{[}\hat {V}'_k.H{]}_{i,j} = 0~\text {(I-type coeff)} \\ {[}V_k.H{]}_{i,j} - [\hat {V}'_k.H]_{i,j} &amp; \text {otherwise}~\text {(P-type coeff)}, \end {array} \right .  \end {equation}

</p><!-- l. 975 --><p class='indent'>   \begin {equation}  [\tilde {E}_k.H] \leftarrow \text {Q}([E_K.H]), \tag {d}  \end {equation}

</p><!-- l. 980 --><p class='indent'>   \begin {equation}  [\tilde {E}_k.H] \leftarrow \text {Q}^{-1}([\tilde {E}_K.H]), \tag {E.g}  \end {equation}

</p><!-- l. 985 --><p class='indent'>   \begin {equation}  [\tilde {V}_k.H] \leftarrow [\tilde {E}_k.H] + [\hat {V}'_k.H], \tag {E.h}  \end {equation}
and notice that if \([\hat {V}_k.H]=0\), then \([\tilde {V}_k.H] = [\tilde {E}_k.H]\),
</p><!-- l. 992 --><p class='indent'>   \begin {equation}  Z^{-1}([\tilde {V}_k.H]) = [V_{k-1}.H], \tag {E.i}  \end {equation}
and by definition, \(Z^{-1}([V_{-1}.H]) = 0\),
</p><!-- l. 998 --><p class='indent'>   \begin {equation}  [\hat {V}_k.H] \leftarrow \text {P}(\overset {k\rightarrow k-1}{V}, [\overset {\sim }{V}_{k-1}.H]), \tag {E.j}  \end {equation}

</p><!-- l. 1008 --><p class='indent'>   \begin {equation}  [\hat {V}'_k.H]_{i,j} \leftarrow \left \{ \begin {array}{ll} {[}\hat {V}_k.H{]}_{i,j} &amp; \text {if}~{[}E_k.L{]}_{i,j} &lt; {[}V_k.L{]}_{i,j} \text {(P-type coeff)} \\ 0 &amp; \text {otherwise (I-type coeff)}, \end {array} \right . \tag {E.k}  \end {equation}

</p><!-- l. 1013 --><p class='indent'>   \begin {equation}  (0, \tilde {E}_k.H) \leftarrow \text {DWT}([\tilde {E}_k.H]), \tag {f}  \end {equation}

</p><!-- l. 1018 --><p class='indent'>   \begin {equation}  \{V_k.L, \tilde {E}_k.H\} \leftarrow \text {E}(V_k.L, \tilde {E}_k.H), \tag {g}  \end {equation}
where E represents the entropy coding of both data sources, in two different
code-streams,
</p><!-- l. 1025 --><p class='indent'>   \begin {equation}  (V_k.L, \tilde {E}_k.H) \leftarrow \text {E}^{-1}(\{V_k.L, \tilde {E}_k.H\}), \tag {h}  \end {equation}

</p><!-- l. 1030 --><p class='indent'>   \begin {equation}  [\tilde {E}_k.H] \leftarrow \text {DWT}^{-1}(0, \tilde {E}_k.H), \tag {i}  \end {equation}

                                                                  

                                                                  
</p><!-- l. 1035 --><p class='indent'>   \begin {equation}  (0, \tilde {V}_k.H) \leftarrow \text {DWT}(0, [\tilde {V}_k.H]), \tag {j}  \end {equation}
</p><!-- l. 1037 --><p class='indent'>   and
</p><!-- l. 1042 --><p class='indent'>   \begin {equation}  \tilde {V}_k \leftarrow \text {DWT}^{-1}(V_k.L, \tilde {V}_k.H). \tag {k}  \end {equation}
</p><!-- l. 1044 --><p class='indent'>   The IPP... codec is inspired in Differential Pulse Code Moldulation. This
<a href='https://github.com/Sistemas-Multimedia/Sistemas-Multimedia.github.io/blob/master/milestones/12-IPP_coding/DPCM.ipynb'>notebook</a> shows how to implement a simple DPCM codec.
</p>
   <h4 class='subsectionHead' id='spatial-multiresolution'><span class='titlemark'>15.3   </span> <a id='x1-4000015.3'></a>Spatial multiresolution</h4>
<!-- l. 1050 --><p class='noindent'>As it can be seen in the previous section, in each IPP... iteration of the step encoder,
only the high-frequency information of the sequence of frames is decorrelated (\(H\)
subbands) considering the information provided by the low-frequences (\(L\) subband),
which are losslessly transmitted between the encoder and the decoder. Notice also
that if the \(L\) data cannot be transmitted to the decoder, a drift error will occur
because the matrix \(S_k\) will be different in the encoder and the decoder for the same
frame \(k\).
</p><!-- l. 1059 --><p class='indent'>   Obviously (and unfortunately), the lossless transmission of the \(L\)’s bounds the
compression ratio that we will get. One solution is to perform more (than 1) levels at
the DWT stage (see Eq. (a)) and to apply the IPP... MRVC step encoder by spatial
resolutions, starting at the lowest, as the decoder will do at decompression time. If we
represent the Spatial Resolution Level (SRL) with an superindex, being 0 the original
SRL, we can express the operation of the codec described in the Fig. <a href='#x1-39001r19'>19<!-- tex4ht:ref: fig:codec  --></a> by
\begin {equation}  \left \{ \begin {array}{l} \text {SE}(V^0_k) = \{V^0_k.L, \tilde {E}^0_k.H\} = \{V^1_k, \tilde {E}^0_k.H\} \\ \text {SD}(\{V^1_k, \tilde {E}^0_k.H\}) = \tilde {V}^0_k, \end {array} \right . \label {eq:codec_1l}  \end {equation}
where \(\text {SE}(\cdot )\) represents to the operation of the IPP... step encoder and \(\text {SD}(\cdot )\) to the operation of
the IPP... step decoder. As it can be seen, Eq. <span class='ecbx-1000'>??</span> is only valid when only one level of
the DWT has been applied.
</p><!-- l. 1081 --><p class='indent'>   In general, for \(s\) levels of the DWT, we have that \begin {equation}  \left \{ \begin {array}{l} \text {SE}(V^{s-1}_k) = \{V^s_k, \tilde {E}^{s-1}_k.H\} \\ \text {SD}(\{V^s_k, \tilde {E}^{s-1}_k.H\}) = \tilde {V}^{s-1}_k, \end {array} \right . \label {eq:codec_sl}  \end {equation}
where \(\tilde {V}^{s-1}\) is the \((s-1)\)-th SRL of the reconstructed sequence \(\tilde {V}\).
</p><!-- l. 1094 --><p class='indent'>   The next SRL (\(s-2\)), \(\tilde {V}^{s-2}\), is determined by \begin {equation}  \left \{ \begin {array}{l} \text {SE}(\tilde {V}^{s-2}_k) = \{\tilde {V}^{s-1}_k, \tilde {E}^{s-2}_k.H\} \\ \text {SD}(\{\tilde {V}^{s-1}_k, \tilde {E}^{s-2}_k.H\}) = \tilde {V}^{s-2}_k, \end {array} \right . \label {eq:codec_s1l}  \end {equation}
and finally, for the highest SRL, we get \(\tilde {V}^0\) defined by Eq. <span class='ecbx-1000'>??</span>.
</p>
   <figure class='figure'> 

                                                                  

                                                                  
                                                                  

                                                                  
<!-- l. 1118 --><p class='noindent' id='-the-ipp-mrvc-encoder-'><div style='text-align:center;'> <img src='graphics/encoder.svg' /> </div>  <a id='x1-40001r20'></a>
<a id='x1-40002'></a>
</p>
<figcaption class='caption'><span class='id'>Figure 20: </span><span class='content'>The IPP... MRVC encoder.                                  </span></figcaption><!-- tex4ht:label?: x1-40001r15  -->
                                                                  

                                                                  
   </figure>
<!-- l. 1123 --><p class='indent'>   So, only the lowest SRL of \(\tilde {V}\), \(\tilde {V}^s\) is an III... pure sequence of small frames, losslessly
encoded. This multiresolution (multistage) scheme has been described in the Fig. <a href='#x1-40001r20'>20<!-- tex4ht:ref: fig:encoder  --></a>.
The output of an IPP... encoder will be refered as “spatial-layers”, or simply as
“S-layers”.
</p><!-- l. 1129 --><p class='indent'>   The Fig. <a href='#x1-40003r21'>21<!-- tex4ht:ref: fig:decoder  --></a> shows the decoder. It inputs the collection of subbands generated by
the encoder and for each one, a video with a different spatial resolution is
obtained.
</p>
   <figure class='figure'> 

                                                                  

                                                                  
                                                                  

                                                                  
<!-- l. 1135 --><p class='noindent' id='-the-ipp-mrvc-decoder-'><div style='text-align:center;'> <img src='graphics/decoder.svg' /> </div>  <a id='x1-40003r21'></a>
<a id='x1-40004'></a>
</p>
<figcaption class='caption'><span class='id'>Figure 21: </span><span class='content'>The IPP... MRVC decoder.                                  </span></figcaption><!-- tex4ht:label?: x1-40003r15  -->
                                                                  

                                                                  
   </figure>
   <h4 class='subsectionHead' id='sq-layers-progression-next-milestone'><span class='titlemark'>15.4   </span> <a id='x1-4100015.4'></a>SQ (layers) progression (next milestone?)</h4>
<!-- l. 1141 --><p class='noindent'>The logical order of the S-layers in the code-stream is the one that allows, when the
code-stream is decoded sequentially, the progressive increase of the spatial resolution
of the video. For example, if \(s\) is the number of levels of the DWT, the generated
code-stream has \((s+1)\) S-layers \begin {equation*}  \{V^s,\tilde {E}^{s-1}.H,\tilde {E}^{s-2}.H,\cdots ,\tilde {E}^0.H\},  \end {equation*}
which are able to generate \((s+1)\) progressive reconstructions \begin {equation*}  \{V^s,\tilde {V}^{s-1},\tilde {V}^{s-2},\cdots ,\tilde {V}^0\}.  \end {equation*}
</p><!-- l. 1155 --><p class='indent'>   Moreover, Quality (Q-progression) scalability in each SRL can be also achieved in
the high-frequency textures if a quality-scalable image codec such as JPEG2000 <span class='cite'>[<a href='#Xtaubman2002jpeg2000'>3</a>]</span>
replaces the PNG compressor, generating a number \(q\) of quality-layers (“Q-layers”) by
each motion compensated high-frequency subband. A SQ-progression is defined
considering both forms of scalability (spatial and quality), with a higher number of
layers. For example, if \(s=3\) (2 IPP...-type iterations) and \(q=2\), the progression of layers would
be \begin {equation*}  \{V^s[1],V^s[0],\tilde {E}^{s-1}.H[1],\tilde {E}^{s-1}.H[0],\tilde {E}^{s-2}.H[1],\tilde {E}^{s-2}.H[0],\cdots ,\tilde {E}^0.H[1],\tilde {E}^0.H[0]\}.  \end {equation*}
</p><!-- l. 1187 --><p class='indent'>   The use of quality scalability boosts the possibilities in real-time streaming scenarios
where the transmission bit-rate can be variable (sending more or less Q-layers of a given
spatial resolution depending on the bit-rate). Notice that the SQ-progression is free of
drift-error.<span class='footnote-mark'><a href='#fn3x0' id='fn3x0-bk'><sup class='textsuperscript'>3</sup></a></span><a id='x1-41001f3'></a>
</p><!-- l. 1205 --><p class='noindent'>
</p>
   <h3 class='sectionHead' id='references'><span class='titlemark'>16   </span> <a id='x1-4200016'></a>References</h3>
   <div class='thebibliography'>
   <p class='bibitem'><span class='biblabel'>
 [1]<span class='bibsp'>   </span></span><a id='Xle1991mpeg'></a>D. Le Gall.   <a href='https://dl.acm.org/doi/pdf/10.1145/103085.103090'>MPEG: A Video Compression Standard for Multimedia
   Applications</a>. <span class='ecti-1000'>Communications of the ACM</span>, 34(4):46–58, 1991.
   </p>
   <p class='bibitem'><span class='biblabel'>
 [2]<span class='bibsp'>   </span></span><a id='Xrao1996techniques'></a>Kamisetty Ramamohan Rao and Jae Jeong Hwang.   <a href='https://scholar.google.es/scholar?hl=es&amp;as_sdt=0%2C5&amp;q=Techniques+and+Standards+for+Image%2C+Video+and+Audio+Coding&amp;btnG='><span class='ecti-1000'>Techniques and
   </span><span class='ecti-1000'>standards for image, video, and audio coding</span></a>, volume 70. Prentice Hall New
   Jersey, 1996.
   </p>
   <p class='bibitem'><span class='biblabel'>
 [3]<span class='bibsp'>   </span></span><a id='Xtaubman2002jpeg2000'></a>D.S. Taubman and W.M. Marcellin.   <a href='https://last.hit.bme.hu/download/firtha/video/JPEG2000/David%20S.%20Taubman,%20%20Michael%20W.%20Marcellin%20%20(auth.)%20JPEG2000%20Image%20Compression%20Fundamentals,%20Standards%20and%20Practice%20%202002.pdf'><span class='ecti-1000'>JPEG2000. Image Compression
   </span><span class='ecti-1000'>Fundamentals, Standards and Practice</span></a>. Kluwer Academic Publishers, 2002.
</p>
                                                                  

                                                                  
   </div>
   <div class='footnotes'><!-- l. 20 --><p class='indent'>     <span class='footnote-mark'><a href='#fn1x0-bk' id='fn1x0'><sup class='textsuperscript'>1</sup></a></span><span class='ecrm-0800'>Notice that at least one SRL is always available for each image or video sequence</span></p>
<!-- l. 877 --><p class='indent'>     <span class='footnote-mark'><a href='#fn2x0-bk' id='fn2x0'><sup class='textsuperscript'>2</sup></a></span><span class='ecrm-0800'>A P-type frame except for the second one, that always has a I-frame as reference.</span></p>
<!-- l. 1194 --><p class='indent'>     <span class='footnote-mark'><a href='#fn3x0-bk' id='fn3x0'><sup class='textsuperscript'>3</sup></a></span><span class='ecrm-0800'>Other progressions such as the QS-progression should generate drift because the decoder at
</span><span class='ecrm-0800'>some truncation points of the code-stream would use a low-frequency information with a different
</span><span class='ecrm-0800'>quality than the encoderd used.</span></p>                                                                                </div>
 
</body> 
</html>